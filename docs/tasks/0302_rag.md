# Retrieval-Augmented Generation (RAG)
Implement a basic RAG pipeline to ground your agent's responses. You may use:

- A small document collection
- Mocked/Generated telco knowledge base 
- Public web available data

Clearly indicate in your response logs when retrieved information is being utilized. Whilst there is limited opportunity to extensively fine-tune, do state some your proposed techniques/approaches to improve the RAG pipeline

## Proposed Design

The RAG component will provide the agent with factual context sourced from a
small knowledge base of telco FAQs and roaming policies. The pipeline will be
built with **LangChain** and use the following components:

1. **Document Loader** – markdown and text files are loaded from `data/` using
   LangChain's generic loaders.
2. **Embeddings Model** – OpenAI embeddings (`text-embedding-3-small`) generate
   vector representations of each document chunk.
3. **Vector Store** – the embeddings are stored in a local **FAISS** index which
   is persisted to disk so it can be reused across runs.
4. **Retriever** – a simple similarity search (`k=4`) fetches the most relevant
   passages for each user query.
5. **Response Synthesis** – retrieved passages are passed to the LLM using
   LangChain's `RetrievalQA` chain which appends citations in the final answer.

### Improvement ideas

- Experiment with cosine similarity thresholding to reduce irrelevant snippets.
- Preprocess text with domain specific synonyms to improve recall.
- Monitor retrieval hits and misses to iteratively expand the document set.

By keeping the dataset small and local we avoid heavy infrastructure while still
demonstrating how the agent can reference up-to-date telco knowledge when
answering user questions.